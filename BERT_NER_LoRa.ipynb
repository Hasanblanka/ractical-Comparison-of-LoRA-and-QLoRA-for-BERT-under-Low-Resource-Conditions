{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad138bfc-0a90-4bf9-b00f-f00421aa47a8",
   "metadata": {},
   "source": [
    "***Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210745e0-e50b-4ebb-b950-019157da6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasan Yusufzada\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "from datasets import Dataset,load_dataset,DatasetDict\n",
    "import peft\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "import seqeval\n",
    "from transformers import pipeline, AutoTokenizer,AutoModelForSequenceClassification,Trainer,TrainingArguments,DataCollatorWithPadding\n",
    "from transformers import AutoModelForQuestionAnswering,BitsAndBytesConfig,AutoModelForTokenClassification,default_data_collator\n",
    "from peft import LoraConfig,get_peft_model,TaskType,PeftModel,prepare_model_for_kbit_training\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import scipy.stats\n",
    "metric_accuracy = evaluate.load('accuracy')\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "metric_seqeval = evaluate.load(\"seqeval\") \n",
    "metric_squad = evaluate.load(\"squad\")\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbfa062-5655-4454-a084-76d7c7f8d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81491d61-3c08-442b-a785-9ee324c28d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e86f004-c159-4062-afd8-ac47e01b21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979f6f1-f720-424c-869d-6fec3ccad40a",
   "metadata": {},
   "source": [
    "***BERT-PEFT-LORA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8440aaf2-1119-46bf-92d9-8694ec8d0781",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "dataset_name = \"conll2003\"\n",
    "low_resource_samples = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957ead22-9f93-4131-99a2-99b19b629c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ner_data():\n",
    "    \n",
    "    dataset = load_dataset(dataset_name)\n",
    "     \n",
    "    train_dataset_small = dataset[\"train\"].select(range(low_resource_samples))\n",
    "    \n",
    "   \n",
    "    label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "   \n",
    "    def tokenize_and_align_labels(examples):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[\"tokens\"], \n",
    "            truncation=True, \n",
    "            is_split_into_words=True,\n",
    "            max_length=512,\n",
    "            padding=False\n",
    "        )\n",
    "        \n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "        \n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "    \n",
    "    \n",
    "    tokenized_train = train_dataset_small.map(tokenize_and_align_labels, batched=True)\n",
    "    tokenized_eval = dataset[\"validation\"].map(tokenize_and_align_labels, batched=True)\n",
    "    \n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer, \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    return tokenized_train, tokenized_eval, data_collator, label_names, tokenizer\n",
    "\n",
    "\n",
    "def compute_ner_metrics(eval_pred, label_names):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i]\n",
    "        label = labels[i]\n",
    "        valid_preds = [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        valid_labels = [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        \n",
    "        true_predictions.append(valid_preds)\n",
    "        true_labels.append(valid_labels)\n",
    "    \n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cfc769e-3d85-4bc1-864c-fa219db4a94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 449,289 || all params: 109,347,858 || trainable%: 0.4109\n"
     ]
    }
   ],
   "source": [
    "tokenized_train, tokenized_eval, data_collator, label_names, tokenizer = prepare_ner_data()\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_names),\n",
    "    id2label={i: label for i, label in enumerate(label_names)},\n",
    "    label2id={label: i for i, label in enumerate(label_names)}\n",
    ")\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.TOKEN_CLS\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_lora\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_ner_lora\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda eval_pred: compute_ner_metrics(eval_pred, label_names)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f401775-1cd8-4eb6-b816-afe97d098df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 42:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.641100</td>\n",
       "      <td>0.572603</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.042578</td>\n",
       "      <td>0.078195</td>\n",
       "      <td>0.838889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.339465</td>\n",
       "      <td>0.509836</td>\n",
       "      <td>0.532144</td>\n",
       "      <td>0.520751</td>\n",
       "      <td>0.913925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.243600</td>\n",
       "      <td>0.271742</td>\n",
       "      <td>0.608805</td>\n",
       "      <td>0.653989</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.930357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.180900</td>\n",
       "      <td>0.248923</td>\n",
       "      <td>0.632164</td>\n",
       "      <td>0.679401</td>\n",
       "      <td>0.654932</td>\n",
       "      <td>0.937094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.196100</td>\n",
       "      <td>0.242319</td>\n",
       "      <td>0.634589</td>\n",
       "      <td>0.689162</td>\n",
       "      <td>0.660750</td>\n",
       "      <td>0.938281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=0.4273364905267954, metrics={'train_runtime': 2533.0159, 'train_samples_per_second': 1.011, 'train_steps_per_second': 0.126, 'total_flos': 52493551377120.0, 'train_loss': 0.4273364905267954, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc85bfaf-6c86-46d8-a68e-1dc64abc8dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [204/204 06:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA-NER {'eval_loss': 0.24231912195682526, 'eval_precision': 0.6345885634588564, 'eval_recall': 0.6891618983507236, 'eval_f1': 0.6607503025413474, 'eval_accuracy': 0.9382812195786768, 'eval_runtime': 374.211, 'eval_samples_per_second': 8.685, 'eval_steps_per_second': 0.545, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"LoRA-NER\", results)\n",
    "\n",
    "trainer.save_model(\"./NER_LoRa_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f35f68-3d28-40c3-bf91-7dd402a6b45b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
